================================
Language Model Fine-tuning
Job ID: 69222
Model: mistral
Selection: random
Data File: Data_Processing/CAI/data/constitutional_training_data_mistral.jsonl
Node: pax007
Started: Sun Nov  9 03:04:31 PM EST 2025
================================

Working Directory set to: /cluster/home/supadh03/CAI/Constituitional-AI
Output directory is: models/finetuned
================================================================================
Constitutional AI Fine-Tuning
================================================================================
Base model: mistralai/Mistral-7B-Instruct-v0.3
Selection method: random
Data file: Data_Processing/CAI/data/constitutional_training_data_mistral.jsonl
Output directory: models/finetuned/mistral_random
Validation size: 79

Loading training data...
Loaded 979 examples
Loading existing split from models/finetuned/train_val_split.json...
✓ Using seed 42
Train: 900 examples
Validation: 79 examples
✓ Validation prompts verified consistent across datasets

Preparing training data...
✓ Training dataset: 900 examples
✓ Validation dataset: 79 examples

Loading mistralai/Mistral-7B-Instruct-v0.3...
✓ CUDA available
✓ GPU: NVIDIA A100 80GB PCIe

✓ Model loaded: cuda:0
✓ Parameters: 7.25B

Tokenizing data...
✓ Tokenization complete

================================================================================
Starting Training
================================================================================

{'loss': 4.5523, 'grad_norm': 59.75, 'learning_rate': 8.079470198675497e-07, 'epoch': 0.89}
{'eval_loss': 7.523564338684082, 'eval_runtime': 7.3794, 'eval_samples_per_second': 10.705, 'eval_steps_per_second': 10.705, 'epoch': 0.89}
{'loss': 14.606, 'grad_norm': 350.25, 'learning_rate': 4.768211920529801e-07, 'epoch': 1.76}
{'eval_loss': 11.109861373901367, 'eval_runtime': 7.3696, 'eval_samples_per_second': 10.72, 'eval_steps_per_second': 10.72, 'epoch': 1.76}
{'loss': 11.0257, 'grad_norm': 86.875, 'learning_rate': 1.4569536423841058e-07, 'epoch': 2.64}
