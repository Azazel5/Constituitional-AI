{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and loading the datasets\n",
        "\n",
        "Download required libraries, set up working directories, load validation data on which we'll be doing further testing on the finetuned models"
      ],
      "metadata": {
        "id": "cbq0yVytdb90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XN2eEwG6JqF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85fbe8f1-24cd-46b6-95bd-a3cd80c21355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes datasets pandas matplotlib \\\n",
        "  sentence-transformers openai ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import ollama\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "1GYNsThnJtCy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFCW2xnNJubX",
        "outputId": "c47aea81-885a-486f-e7ad-f6aafdee86e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GDRIVE_MODEL_DIR = Path(\"/content/drive/MyDrive/Constitutional AI\")\n",
        "DATA_DIR = GDRIVE_MODEL_DIR / \"Data\"\n",
        "\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    \"qwen_random\": {\n",
        "        \"path\": os.path.join(GDRIVE_MODEL_DIR, \"qwen_random\"),\n",
        "        \"data_file\": GDRIVE_MODEL_DIR / \"Data/constitutional_training_data_qwen.jsonl\",\n",
        "        \"family\": \"qwen\",\n",
        "    },\n",
        "    \"qwen_contextual\": {\n",
        "        \"path\": os.path.join(GDRIVE_MODEL_DIR, \"qwen_contextual\"),\n",
        "        \"data_file\": GDRIVE_MODEL_DIR / \"Data/constitutional_training_data_qwen-contextual-True.jsonl\",\n",
        "        \"family\": \"qwen\"\n",
        "    },\n",
        "    \"mistral_random\": {\n",
        "        \"path\": os.path.join(GDRIVE_MODEL_DIR, \"mistral_random\"),\n",
        "          \"data_file\": GDRIVE_MODEL_DIR / \"Data/constitutional_training_data_mistral.jsonl\",\n",
        "        \"family\": \"mistral\"\n",
        "    },\n",
        "    \"mistral_contextual\": {\n",
        "        \"path\": os.path.join(GDRIVE_MODEL_DIR, \"mistral_contextual\"),\n",
        "        \"data_file\": GDRIVE_MODEL_DIR / \"Data/constitutional_training_data_mistral-contextual-True.jsonl\",\n",
        "        \"family\": \"mistral\"\n",
        "    }\n",
        "}\n",
        "\n",
        "GAUGE_CONFIG = {\n",
        "    \"helpfulness_rm\": \"berkeley-nest/Starling-RM-7B-alpha\",\n",
        "    \"harmlessness_classifier\": \"facebook/roberta-hate-speech-dynabench-r4-target\",\n",
        "    \"similarity_model\": \"all-MiniLM-L6-v2\"\n",
        "}"
      ],
      "metadata": {
        "id": "tKc0zVp1JvzJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OLLAMA_JUDGE_MODEL = \"llama3\"\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "OUTPUT_DIR = GDRIVE_MODEL_DIR / \"evaluation_results\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Configuration loaded. Models will be loaded from: {GDRIVE_MODEL_DIR}\")\n",
        "print(f\"Evaluations saved to: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKylmI8iU6eA",
        "outputId": "8eda38c6-b939-461d-9ecf-99acd9e6b98c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded. Models will be loaded from: /content/drive/MyDrive/Constitutional AI\n",
            "Evaluations saved to: /content/drive/MyDrive/Constitutional AI/evaluation_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate responses from finetuned models"
      ],
      "metadata": {
        "id": "CFzIFyCKeCpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, config in MODEL_CONFIG.items():\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Generating responses for model: {model_name}\")\n",
        "\n",
        "    print(f\"Loading examples using: {config['data_file']}\")\n",
        "    all_examples = []\n",
        "\n",
        "    with open(config['data_file'], 'r') as f:\n",
        "      for line in f:\n",
        "        all_examples.append(json.loads(line))\n",
        "\n",
        "    split_file = os.path.join(DATA_DIR, \"train_val_split.json\")\n",
        "\n",
        "    with open(split_file, 'r') as f:\n",
        "      split_info = json.load(f)\n",
        "\n",
        "    val_indices = split_info['val_indices']\n",
        "    print(f\"Loaded {len(val_indices)} validation indices.\")\n",
        "\n",
        "    validation_set = [all_examples[i] for i in val_indices]\n",
        "    validation_prompts = [ex['prompt'] for ex in validation_set]\n",
        "    ground_truth_responses = [ex['revision'] for ex in validation_set]\n",
        "\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "    model = AutoModelForCausalLM.from_pretrained(config['path'], quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=(config['family'] == \"qwen\"))\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['path'], trust_remote_code=(config['family'] == \"qwen\"))\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    generated_responses = []\n",
        "\n",
        "\n",
        "    for i, prompt in enumerate(validation_prompts):\n",
        "        if i % 20 == 0:\n",
        "            print(f\"  ...generating response {i+1}/{len(validation_prompts)}\")\n",
        "\n",
        "\n",
        "        inputs = tokenizer(f\"Human: {prompt}\\n\\nAssistant:\", return_tensors=\"pt\").to(\"cuda\").input_ids\n",
        "\n",
        "\n",
        "        outputs = model.generate(inputs, max_new_tokens=256, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id)\n",
        "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True).strip()\n",
        "        generated_responses.append(response)\n",
        "\n",
        "\n",
        "    df_responses = pd.DataFrame({\n",
        "        \"prompt\": validation_prompts,\n",
        "        \"ground_truth_response\": ground_truth_responses,\n",
        "        \"generated_response\": generated_responses\n",
        "    })\n",
        "\n",
        "\n",
        "    output_path = os.path.join(OUTPUT_DIR, f\"{model_name}_responses.csv\")\n",
        "    df_responses.to_csv(output_path, index=False)\n",
        "    print(f\"✅ Saved 79 responses to {output_path}\")\n",
        "\n",
        "    del model\n",
        "    del tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Cleared VRAM for model: {model_name}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- PHASE 1 COMPLETE: All responses generated and saved. ---\")"
      ],
      "metadata": {
        "id": "ETrxG8l3hh95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oT16-vRpmdW4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}